[
<<<<<<< HEAD
["exploratory-data-analysis.html", "Chapter 4 Exploratory Data Analysis 4.1 Network Analysis 4.2 E-mails Analysis 4.3 Locations 4.4 Searches", " Chapter 4 Exploratory Data Analysis Once the often laborious task of data manipulation is complete, the next step in the process is to become intimately familiar with the data set by performing Exploratory Data Analysis (EDA). The way to gain this level of understanding is to utilize the features of the statistical environment one is using that support this effort. It is always a good idea to explore a data set with multiple exploratory techniques, especially when they can be done together for comparison, we did not have a large amount of feaures but and important amount of effort to merge the data. Then, it is quite possible that one may need to revisit one or more data manipulation tasks in order to refine or transform the data even further. The goal of exploratory data analysis is to obtain confidence in the data to a point where one is ready to engage a machine learning algorithms. see EDA 4.1 Network Analysis What if becasuse of my networking one is tended to make different choices?, Is it the same relevance that one send ten e-mails to ten persons, than send ten e-mails to two persons? Well it is quite difficult to see at the first gaze, and maybe we have to make a deeper in-degrees and out-degrees analysis on e-mail networking. graph Let’s find the biggest email threads, once we got it, we visualize the longest path, using cypher query language Biggest email threads MATCH p=(e:Email)&lt;-[:REPLY*]-(r:Email)&lt;-[]-(sender:Account) WHERE NOT (e)-[:REPLY]-&gt;() RETURN sender.name, e.subject, Id(e), length(p) - 1 AS depth ORDER BY depth DESC LIMIT 100 // Email thread by email id MATCH p=(n:Email)&lt;-[:REPLY*]-(:Email) WHERE id(n)=123 RETURN p longest To achive a database based on graph we used a parser builded on Go programing language, helped on a .mbox reader builded in Perl A little summary of email2neo4jpackage: mbox2neo4j is go command line tools that allow you to import your emails from an mbox file into a neo4j graph database. It used a model from the email example project that is explained in Chapter 3 of the book Graph Databases by Ian Robinson, Jim Webber and Emil Eifrem (which is a truly great introduction to the graph btw) First we have to take the attachments of the e-mails On terminal: perl strip-attachments.pl /path/to/mbox/file.mbox Then mbox2neo4j /path/to/mbox/file localhost:7474 4.2 E-mails Analysis It is important to know the distribution of the emailing traffic, then is suitable to know when recommend, as we see before the more we know the user data the accurate the recommendation. The e-mail traffic analysis through the time is partially based on Geoff Boeing work, and it is been developed on python import mailbox, pandas as pd, numpy as np import matplotlib.pyplot as plt, matplotlib.font_manager as fm from dateutil.parser import parse as parse_datetime %matplotlib inline # define the fonts to use for plots #family = &#39;Myriad Pro&#39; family = &#39;serif&#39; title_font = fm.FontProperties(family=family, style=&#39;normal&#39;, size=20, weight=&#39;normal&#39;, stretch=&#39;normal&#39;) label_font = fm.FontProperties(family=family, style=&#39;normal&#39;, size=16, weight=&#39;normal&#39;, stretch=&#39;normal&#39;) ticks_font = fm.FontProperties(family=family, style=&#39;normal&#39;, size=12, weight=&#39;normal&#39;, stretch=&#39;normal&#39;) Load the Gmail archive and parse dates/times from messages # load the mbox file #path = &#39;Destacados.mbox&#39; path = &#39;/Users/pedrohserrano/google-takeout/Mail/Enviados.mbox&#39; mbox = mailbox.mbox(path) print(&#39;There are {:,} messages in the archive.&#39;.format(len(mbox))) There are 1,699 messages in the archive. The Gmail mbox file includes emails and hangouts chats among its “messages”. Hangouts messages don’t have date/time, so we’ll only parse dates and times from the actual emails, and just ignore the hangouts chats. Also, some chats do have a date. To filter them out, verify that if the message has a label that the label does not include “Chat”. # get a list of the dates/times of all the messages in the mbox all_dates = [] all_times = [] for message in mbox: # it&#39;s an email and not a chat if there&#39;s no label, or if there&#39;s a label but it&#39;s not &#39;chat&#39; if not &#39;X-Gmail-Labels&#39; in message or (&#39;X-Gmail-Labels&#39; in message and not &#39;Chat&#39; in message[&#39;X-Gmail-Labels&#39;]): if &#39;Date&#39; in message and message[&#39;Date&#39;] is not None: try: date, time = str(parse_datetime(message[&#39;Date&#39;])).split(&#39; &#39;) except Exception as e: print(e, message[&#39;Date&#39;]) all_dates.append(date) all_times.append(time) else: # hangouts messages have no Date key, so skip them pass print(&#39;There are {:,} messages with dates.&#39;.format(len(all_dates))) There are 1,699 messages with dates. Plot the mail traffic by date¶ # get the count per date date_counts = pd.Series(all_dates).value_counts().sort_index() print(&#39;There are {:,} dates with messages.&#39;.format(len(date_counts))) date_counts.head() # not every date necessarily has a message, so fill in missing dates in the range with zeros date_range = pd.date_range(start=min(all_dates), end=max(all_dates), freq=&#39;D&#39;) index = date_range.map(lambda x: str(x.date())) date_counts = date_counts.reindex(index, fill_value=0) print(&#39;There are {:,} dates total in the range, with or without messages.&#39;.format(len(date_counts))) date_counts.head() # create a series of labels for the plot: each new year&#39;s day xlabels = pd.Series([label if &#39;01-01&#39; in label else None for label in date_counts.index]) xlabels = xlabels[pd.notnull(xlabels)] xlabels.head() # plot the counts per day fig = plt.figure(figsize=[15, 5]) ax = date_counts.plot(kind=&#39;line&#39;, linewidth=0.5, alpha=0.5, color=&#39;g&#39;) ax.grid(True) ax.set_xticks(xlabels.index) ax.set_xticklabels(xlabels, rotation=35, rotation_mode=&#39;anchor&#39;, ha=&#39;right&#39;, fontproperties=ticks_font) ax.set_ylabel(&#39;Number of emails&#39;, fontproperties=label_font) ax.set_title(&#39;Sent mails traffic per day&#39;, fontproperties=title_font) fig.tight_layout() fig.savefig(&#39;images/gmail-traffic-day-destacados.png&#39;, dpi=96) plt.show() gmail-traffic-day-destacados Plot the traffic month by month # get the count per month all_months = [x[:-3] for x in all_dates] month_counts = pd.Series(all_months).value_counts().sort_index() # not every month necessarily has a message, so fill in missing months in the range with zeros date_range = pd.date_range(start=min(all_dates), end=max(all_dates), freq=&#39;D&#39;) months_range = date_range.map(lambda x: str(x.date())[:-3]) index = np.unique(months_range) month_counts = month_counts.reindex(index, fill_value=0) # create a series of labels for the plot: each january xlabels = pd.Series([label if &#39;-01&#39; in label else None for label in month_counts.index]) xlabels = xlabels[pd.notnull(xlabels)] xlabels.head() # plot the counts per month fig = plt.figure(figsize=[15, 5]) ax = month_counts.plot(kind=&#39;line&#39;, linewidth=2.5, alpha=0.6, color=&#39;g&#39;, marker=&#39;+&#39;, markeredgecolor=&#39;g&#39;) ax.grid(True) ax.set_xticks(xlabels.index) ax.set_xticklabels(xlabels, rotation=35, rotation_mode=&#39;anchor&#39;, ha=&#39;right&#39;, fontproperties=ticks_font) ax.set_ylabel(&#39;Number of emails&#39;, fontproperties=label_font) ax.set_title(&#39;Sent mail traffic per month&#39;, fontproperties=title_font) fig.tight_layout() fig.savefig(&#39;images/gmail-traffic-month.png&#39;, dpi=96) plt.show() gmail-traffic-month 4.2.1 Plot the mail traffic by the day of the week # get the count per day of the week day_counts = pd.DataFrame() day_counts[&#39;count&#39;] = date_counts day_counts[&#39;day_of_week&#39;] = date_counts.index.map(lambda x: parse_datetime(x).weekday()) mean_day_counts = day_counts.groupby(&#39;day_of_week&#39;)[&#39;count&#39;].mean() xlabels = [&#39;Monday&#39;, &#39;Tuesday&#39;, &#39;Wednesday&#39;, &#39;Thursday&#39;, &#39;Friday&#39;, &#39;Saturday&#39;, &#39;Sunday&#39;] fig = plt.figure(figsize=[15, 5]) ax = mean_day_counts.plot(kind=&#39;bar&#39;, width=0.6, alpha=0.5, color=&#39;g&#39;, edgecolor=&#39;#333333&#39;) ax.yaxis.grid(True) ax.set_xticklabels(xlabels, rotation=35, rotation_mode=&#39;anchor&#39;, ha=&#39;right&#39;, fontproperties=ticks_font) for label in ax.get_yticklabels(): label.set_fontproperties(ticks_font) ax.set_title(&#39;Sent mails traffic by day of the week&#39;, fontproperties=title_font) ax.set_xlabel(&#39;&#39;) ax.set_ylabel(&#39;Mean number of emails&#39;, fontproperties=label_font) fig.tight_layout() fig.savefig(&#39;images/gmail-traffic-day-week.png&#39;, dpi=96) plt.show() gmail-traffic-day-week 4.2.2 Plot the mail traffic by the hour of the day # get the count per hour of the day times = pd.Series(all_times).map(lambda x: &#39;{:02}:00&#39;.format(parse_datetime(x).hour)) time_counts = times.value_counts().sort_index() time_counts.head() fig = plt.figure(figsize=[15, 5]) ax = time_counts.plot(kind=&#39;bar&#39;, width=0.8, alpha=0.5, color=&#39;g&#39;, edgecolor=&#39;#333333&#39;) ax.yaxis.grid(True) ax.set_xticklabels(time_counts.index, rotation=45, rotation_mode=&#39;anchor&#39;, ha=&#39;right&#39;, fontproperties=ticks_font) for label in ax.get_yticklabels(): label.set_fontproperties(ticks_font) ax.set_title(&#39;Sent mails traffic by hour of the day&#39;, fontproperties=title_font) ax.set_ylabel(&#39;Number of emails&#39;, fontproperties=label_font) fig.tight_layout() fig.savefig(&#39;images/gmail-traffic-hour.png&#39;, dpi=96) plt.show() gmail-traffic-hour 4.2.3 Plot the mail traffic by the minute of the day # get the count per minute of the day, as hh:mm minutes = pd.Series(all_times).map(lambda x: &#39;{:02}:{:02}&#39;.format(parse_datetime(x).hour, parse_datetime(x).minute)) minute_counts = minutes.value_counts().sort_index() # not every minute necessarily has a message, so fill in missing times with zeros time_range = pd.date_range(start=&#39;0:00&#39;, end=&#39;23:59&#39;, freq=&#39;1min&#39;) index = time_range.map(lambda x: &#39;{:02}:{:02}&#39;.format(x.hour, x.minute)) minute_counts = minute_counts.reindex(index, fill_value=0) # create a series of labels for the plot: each new hour xlabels = pd.Series([label if &#39;:00&#39; in label else None for label in minute_counts.index]) xlabels = xlabels[pd.notnull(xlabels)] # plot the counts per minute fig = plt.figure(figsize=[15, 5]) ax = minute_counts.plot(kind=&#39;line&#39;, linewidth=0.7, alpha=0.7, color=&#39;g&#39;) ax.grid(True) ax.set_xticks(xlabels.index) ax.set_xticklabels(xlabels, rotation=45, rotation_mode=&#39;anchor&#39;, ha=&#39;right&#39;, fontproperties=ticks_font) ax.set_ylabel(&#39;Number of emails&#39;, fontproperties=label_font) ax.set_title(&#39;Sent mails traffic by minute of the day&#39;, fontproperties=title_font) fig.tight_layout() fig.savefig(&#39;images/gmail-traffic-minute.png&#39;, dpi=96) plt.show() gmail-traffic-hour As we can see here if we want to develop a recomender system around this user better we send ads based on how likely the user is online, we can now that when he/she is writing emails down For this user the most likely time is around 10 to eleven and 16 to 17, Tuesdays and Thursdays, and also is important to say that there’s such an active account between November and February. gmail-traffic-hour 4.3 Locations The main input we used is the location history file, that is because the the application is going to recomend the sites of interest around the most visited places. In other words we seek to describe the daily activity of the person and define their most frequent places (home, office or school). During the day, a person performs two types of activities: staying in one place or moving to another. When a person visits a place, they tend to stay at that location for at least 10 minutes (can be extended to hours in home, office or school cases) which generates at least 17 location records. When the person moves from one place to another, a series of location records are generated along the transited space. These records may or may not be close (geographically) depending on the transfer speed. If you are in a congestion, it is possible, although unlikely, to confuse the place as a visit (depending on the tolerance we take for each visit). As a result of the day, location records generate a cloud of points with areas of high and low density. In order to define the places visited during the day, it is proposed to use the DBSCAN Model which is an unsupervised clusterization algorithm based on density. When we used, the DBSCAN, unclassified points and points clusters are obtained. It will be assumed that points within a cluster are variations due to GPS accuracy or internal movements within the building. Also, within the clusters are the last records obtained during the displacement towards said cluster and the first records of the transfer to the next place. In order to define the visited location (the main one), the average of the records of each cluster will be taken excluding the first and the last decile in order to reduce the error caused by clustering. The unlabeled points correspond to transfer location records. Also was quite hard to define the frequently places, so we used a window of 30 days in order to detect if the period of analysis is a common period or a change of address (work or school) or a holiday period. Using the data of the volunteers for this study, we found outthat there are approximately 816 registered locations per day, approximately 34 records per hour or one record every 34 seconds. import numpy as np from sklearn.cluster import DBSCAN from sklearn import metrics #from sklearn.datasets.samples_generator import make_blobs from sklearn.preprocessing import StandardScaler import pandas as pd import json import simplejson import datetime import calendar from urllib.request import urlopen,quote import os import webbrowser #import operator #with open(&#39;Historialdeubicaciones.json&#39;, &#39;r&#39;) as fh: with open(&#39;LocationHistory2.json&#39;, &#39;r&#39;) as fh: raw = json.loads(fh.read()) ld = pd.DataFrame(raw[&#39;locations&#39;]) file = open(&quot;dia.csv&quot;,&quot;w&quot;) for i in range(len(ld)): file.write(&quot;{0:.7f}&quot;.format(ld[&#39;latitudeE7&#39;][i]/10000000)+&quot;,&quot;+&quot;{0:.7f}&quot;.format(ld[&#39;longitudeE7&#39;][i]/10000000)+&#39;,&#39;+ld[&#39;timestampMs&#39;][i]+&#39;,&#39;+ datetime.datetime.fromtimestamp( int(ld[&#39;timestampMs&#39;][i])/ 1e3 ).strftime(&#39;%Y-%m-%d&#39;)+&#39;,&#39;+datetime.datetime.fromtimestamp( int(ld[&#39;timestampMs&#39;][i])/ 1e3 ).strftime(&#39;%H:%M:%S&#39;) +&#39;,&#39; +calendar.day_name[datetime.datetime.fromtimestamp(int(ld[&#39;timestampMs&#39;][i])/ 1e3 ).weekday()]+ &#39;\\n&#39;) file.close() coords=pd.read_csv(&#39;dia.csv&#39;, names = [&quot;lat&quot;, &quot;lon&quot;,&quot;timestamp&quot;,&quot;fecha&quot;,&quot;hora&quot;,&quot;dia&quot;]) Define frequent places: 30 day window coords3=coords[coords.fecha==coords[&#39;fecha&#39;].unique()[0]] hours=[&#39;00:00:00&#39;,&#39;01:00:00&#39;, &#39;02:00:00&#39;,&#39;03:00:00&#39;, &#39;04:00:00&#39;,&#39;05:00:00&#39;, &#39;06:00:00&#39;,&#39;07:00:00&#39;, &#39;08:00:00&#39;,&#39;09:00:00&#39;, &#39;10:00:00&#39;,&#39;11:00:00&#39;, &#39;12:00:00&#39;,&#39;13:00:00&#39;, &#39;14:00:00&#39;,&#39;15:00:00&#39;, &#39;16:00:00&#39;,&#39;17:00:00&#39;, &#39;18:00:00&#39;,&#39;19:00:00&#39;, &#39;20:00:00&#39;,&#39;21:00:00&#39;, &#39;22:00:00&#39;,&#39;23:00:00&#39;,&#39;23:59:59&#39;] inicio=0 final=10 for i in range(inicio,final): coords3=coords3.append(coords[coords.fecha==coords[&#39;fecha&#39;].unique()[1+i]]) print(coords[&#39;fecha&#39;].unique()[1+inicio],coords[&#39;fecha&#39;].unique()[1+final]) cosa=coords3[[&#39;lat&#39;,&#39;lon&#39;]] cosa = cosa.reset_index(drop=True) min_samples=np.max([20,len(cosa)*.07]) scaler = StandardScaler() scaler.fit(cosa) X=scaler.fit_transform(cosa) direcciones={} db = DBSCAN(eps=0.031, min_samples=min_samples).fit(X) core_samples_mask = np.zeros_like(db.labels_, dtype=bool) core_samples_mask[db.core_sample_indices_] = True labels = db.labels_ cosa=cosa.iloc[db.core_sample_indices_] cosa = cosa.reset_index(drop=True) recuento={} # Number of clusters in labels, ignoring noise if present. n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0) import matplotlib.pyplot as plt unique_labels = set(labels) colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels))) clusters = [X[labels == i] for i in range(n_clusters_)] markers=&quot;&quot;&quot; &quot;&quot;&quot; places=&quot;AIzaSyCsgMwi_tzAVkae-8Rq9v2A_kjeJF5L2kU&quot; c0=scaler.inverse_transform(clusters[0]) c0r=pd.DataFrame(data=c0[0:,0:]) c0r.columns = [&#39;lat&#39;, &#39;lon&#39;] c0r[&#39;cluster&#39;]=0 casa={} matutino={} for i in range(n_clusters_): c0=scaler.inverse_transform(clusters[i]) c0r=pd.DataFrame(data=c0[0:,0:]) c0r.columns = [&#39;lat&#39;, &#39;lon&#39;] c0r[&#39;cluster&#39;]=i aux=c0r.drop_duplicates() aux=aux.reset_index(drop=True) horas=coords3[(coords3[&#39;lat&#39;]==aux[&#39;lat&#39;].loc[0]) &amp;(coords3[&#39;lon&#39;]==aux[&#39;lon&#39;].loc[0])] casa[i]=0 matutino[i]=0 vespertino=0 diurno=0 for k in range(1,len(aux)): horas=horas.append(coords3[(coords3[&#39;lat&#39;]==aux[&#39;lat&#39;].loc[k]) &amp;(coords3[&#39;lon&#39;]==aux[&#39;lon&#39;].loc[k])]) cosita=datetime.datetime.strptime(np.max(horas[&#39;hora&#39;]),&#39;%H:%M:%S&#39;)-datetime.datetime.strptime(np.min(horas[&#39;hora&#39;]),&#39;%H:%M:%S&#39;) maximo=datetime.datetime.strptime(np.max(horas[&#39;hora&#39;]),&#39;%H:%M:%S&#39;)-cosita*0 minimo=datetime.datetime.strptime(np.min(horas[&#39;hora&#39;]),&#39;%H:%M:%S&#39;)+cosita*0 bajo=&#39;00:00:00&#39; alto=&#39;00:00:00&#39; for alto in hours: temp=horas[(horas[&#39;hora&#39;]&lt;alto)&amp;(horas[&#39;hora&#39;]&gt;bajo)] recuento[alto]=len(temp[&#39;hora&#39;]) if ((alto&lt;&#39;07:00:00&#39;)|(alto&gt;&#39;23:00:00&#39;)): casa[i]=casa[i]+len(temp[&#39;hora&#39;]) if ((alto&lt;&#39;17:00:00&#39;)|(alto&gt;&#39;11:00:00&#39;)): matutino[i]=matutino[i]+len(temp[&#39;hora&#39;]) bajo=alto util=horas[(horas[&#39;hora&#39;]&lt;maximo.strftime(&quot;%H:%M:%S&quot;))&amp;(horas[&#39;hora&#39;]&gt;minimo.strftime(&quot;%H:%M:%S&quot;))] if(len(util)&gt;0): lon= np.mean(util[&#39;lon&#39;]) lat= np.mean(util[&#39;lat&#39;]) url_maps=&quot;https://maps.googleapis.com/maps/api/geocode/json?latlng=&quot;+str(lat)+&quot;,&quot;+str(lon)+&quot;&amp;key=AIzaSyCb0Wakn29V87eBdMd_fAb3DGcxAKtqtxY&quot; with urlopen(url_maps) as response: result= simplejson.load(urlopen(url_maps)) direcciones[i]=result[&#39;results&#39;][0][&#39;formatted_address&#39;] url_places1=&quot;https://maps.googleapis.com/maps/api/place/nearbysearch/json?location=&quot;+str(lat)+&quot;,&quot;+str(lon)+&quot;&amp;rankby=distance&quot;+&quot;&amp;types=None&quot;+&quot;&amp;key=&quot;+places markers=markers+&quot;&quot;&quot;var marker = new google.maps.Marker({ map: map, draggable: true, icon: { path: google.maps.SymbolPath.CIRCLE, scale:5 }, position: {lat: &quot;&quot;&quot;+ str(lat) +&quot;&quot;&quot; , lng: &quot;&quot;&quot;+str(lon)+&quot;&quot;&quot;}, title: &#39;&quot;&quot;&quot;+result[&#39;results&#39;][0][&#39;formatted_address&#39;]+&quot;&quot;&quot;cluster: &quot;&quot;&quot;+str(i)+&quot;&quot;&quot;&#39; });&quot;&quot;&quot; centro=&#39;{lat:&#39;+ str(np.mean(cosa[&#39;lat&#39;])) +&quot;&quot;&quot; , lng: &quot;&quot;&quot;+str(np.mean(cosa[&#39;lon&#39;]))+&#39;}&#39; print(casa) print(matutino) print(direcciones) 2017-03-21 2017-03-11 {0: 0, 1: 914, 2: 268, 3: 0} {0: 1386, 1: 3333, 2: 830, 3: 1040} {0: &#39;Edificio 10, Altavista, Ciudad de México, CDMX, Mexico&#39;, 1: &#39;Cerro San Francisco 305, Campestre Churubusco, 04200 Ciudad de México, CDMX, Mexico&#39;, 2: &#39;Cto. Interior Maestro José Vasconcelos 208, Condesa, 06140 Ciudad de México, CDMX, Mexico&#39;, 3: &#39;Torre C, Av Sta Fe 505, Santa Fe, Contadero, 01219 Ciudad de México, CDMX, Mexico&#39;} #print(&#39;Trabajo: &#39;,direcciones[max(matutino, key=matutino.get)]) aux=[k for k, v in casa.items() if v &gt; 0.4*sum(casa.values())] for i in aux: print(&#39;Casa &#39;,i,&#39;: &#39;,direcciones[i]) aux=[k for k, v in matutino.items() if v &gt; sum(matutino.values())/(n_clusters_+1)] for i in aux: print(&#39;Trabajo/Escuela &#39;,i,&#39;: &#39;,direcciones[i]) Casa 1 : Cerro San Francisco 305, Campestre Churubusco, 04200 Ciudad de México, CDMX, Mexico Trabajo/Escuela 0 : Edificio 10, Altavista, Ciudad de México, CDMX, Mexico Trabajo/Escuela 1 : Cerro San Francisco 305, Campestre Churubusco, 04200 Ciudad de México, CDMX, Mexico direcciones Day #with open(&#39;Historialdeubicaciones.json&#39;, &#39;r&#39;) as fh: with open(&#39;LocationHistory2.json&#39;, &#39;r&#39;) as fh: raw = json.loads(fh.read()) ld = pd.DataFrame(raw[&#39;locations&#39;]) file = open(&quot;dia.csv&quot;,&quot;w&quot;) for i in range(len(ld)): file.write(&quot;{0:.7f}&quot;.format(ld[&#39;latitudeE7&#39;][i]/10000000)+&quot;,&quot;+&quot;{0:.7f}&quot;.format(ld[&#39;longitudeE7&#39;][i]/10000000)+&#39;,&#39;+ld[&#39;timestampMs&#39;][i]+&#39;,&#39;+ datetime.datetime.fromtimestamp( int(ld[&#39;timestampMs&#39;][i])/ 1e3 ).strftime(&#39;%Y-%m-%d&#39;)+&#39;,&#39;+datetime.datetime.fromtimestamp( int(ld[&#39;timestampMs&#39;][i])/ 1e3 ).strftime(&#39;%H:%M:%S&#39;) +&#39;,&#39; +calendar.day_name[datetime.datetime.fromtimestamp(int(ld[&#39;timestampMs&#39;][i])/ 1e3 ).weekday()]+ &#39;\\n&#39;) file.close() coords=pd.read_csv(&#39;dia.csv&#39;, names = [&quot;lat&quot;, &quot;lon&quot;,&quot;timestamp&quot;,&quot;fecha&quot;,&quot;hora&quot;,&quot;dia&quot;]) coords2=coords[coords.fecha==&#39;2017-02-02&#39;] cosa=coords2[[&#39;lat&#39;,&#39;lon&#39;]] cosa = cosa.reset_index(drop=True) print(len(cosa)) min_samples=np.max([20,len(cosa)*.05]) print(&#39;min&#39;,min_samples) scaler = StandardScaler() scaler.fit(cosa) X=scaler.fit_transform(cosa) db = DBSCAN(eps=0.085, min_samples=min_samples).fit(X) core_samples_mask = np.zeros_like(db.labels_, dtype=bool) core_samples_mask[db.core_sample_indices_] = True print(len(db.core_sample_indices_)) labels = db.labels_ cosa=cosa.iloc[db.core_sample_indices_] cosa = cosa.reset_index(drop=True) # Number of clusters in labels, ignoring noise if present. n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0) print(&#39;Estimated number of clusters: %d&#39; % n_clusters_) import matplotlib.pyplot as plt unique_labels = set(labels) colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels))) for k, col in zip(unique_labels, colors): if k == -1: # Black used for noise. col = &#39;k&#39; class_member_mask = (labels == k) xy = X[class_member_mask &amp; core_samples_mask] plt.plot(xy[:, 0], xy[:, 1], &#39;o&#39;, markerfacecolor=col, markeredgecolor=&#39;k&#39;, markersize=7) xy = X[class_member_mask &amp; ~core_samples_mask] plt.plot(xy[:, 0], xy[:, 1], &#39;o&#39;, markerfacecolor=col, markeredgecolor=&#39;k&#39;, markersize=2) plt.title(&#39;Estimated number of clusters: %d&#39; % n_clusters_) plt.axis(&#39;off&#39;) plt.show() clusters = [X[labels == i] for i in range(n_clusters_)] markers=&quot;&quot;&quot; &quot;&quot;&quot; places=&quot;AIzaSyCsgMwi_tzAVkae-8Rq9v2A_kjeJF5L2kU&quot; c0=scaler.inverse_transform(clusters[0]) c0r=pd.DataFrame(data=c0[0:,0:]) c0r.columns = [&#39;lat&#39;, &#39;lon&#39;] c0r[&#39;cluster&#39;]=0 intento=c0r for i in range(n_clusters_): c0=scaler.inverse_transform(clusters[i]) c0r=pd.DataFrame(data=c0[0:,0:]) c0r.columns = [&#39;lat&#39;, &#39;lon&#39;] c0r[&#39;cluster&#39;]=i intento=intento.append(c0r) aux=c0r.drop_duplicates() aux=aux.reset_index(drop=True) horas=coords2[(coords2[&#39;lat&#39;]==aux[&#39;lat&#39;].loc[0]) &amp;(coords2[&#39;lon&#39;]==aux[&#39;lon&#39;].loc[0])] for k in range(1,len(aux)): horas=horas.append(coords2[(coords2[&#39;lat&#39;]==aux[&#39;lat&#39;].loc[k]) &amp;(coords2[&#39;lon&#39;]==aux[&#39;lon&#39;].loc[k])]) cosita=datetime.datetime.strptime(np.max(horas[&#39;hora&#39;]),&#39;%H:%M:%S&#39;)-datetime.datetime.strptime(np.min(horas[&#39;hora&#39;]),&#39;%H:%M:%S&#39;) maximo=datetime.datetime.strptime(np.max(horas[&#39;hora&#39;]),&#39;%H:%M:%S&#39;)-cosita*.1 minimo=datetime.datetime.strptime(np.min(horas[&#39;hora&#39;]),&#39;%H:%M:%S&#39;)+cosita*.1 print (i,maximo,minimo) util=horas[(horas[&#39;hora&#39;]&lt;maximo.strftime(&quot;%H:%M:%S&quot;))&amp;(horas[&#39;hora&#39;]&gt;minimo.strftime(&quot;%H:%M:%S&quot;))] if(len(util)&gt;0): lon= np.mean(util[&#39;lon&#39;]) lat= np.mean(util[&#39;lat&#39;]) url_maps=&quot;https://maps.googleapis.com/maps/api/geocode/json?latlng=&quot;+str(lat)+&quot;,&quot;+str(lon)+&quot;&amp;key=AIzaSyCb0Wakn29V87eBdMd_fAb3DGcxAKtqtxY&quot; with urlopen(url_maps) as response: result= simplejson.load(urlopen(url_maps)) print (result[&#39;results&#39;][0][&#39;formatted_address&#39;]) url_places1=&quot;https://maps.googleapis.com/maps/api/place/nearbysearch/json?location=&quot;+str(lat)+&quot;,&quot;+str(lon)+&quot;&amp;rankby=distance&quot;+&quot;&amp;types=None&quot;+&quot;&amp;key=&quot;+places #with urlopen(url_places1) as response: # result_p1= simplejson.load(urlopen(url_places1)) #print (&quot;están en:&quot;,result_p1[&#39;results&#39;][0][&#39;name&#39;],&#39;---&#39;,result_p1[&#39;results&#39;][0][&#39;types&#39;][0]) markers=markers+&quot;&quot;&quot;var marker = new google.maps.Marker({ map: map, draggable: true, icon: { path: google.maps.SymbolPath.CIRCLE, scale:5 }, position: {lat: &quot;&quot;&quot;+ str(lat) +&quot;&quot;&quot; , lng: &quot;&quot;&quot;+str(lon)+&quot;&quot;&quot;}, title: &#39;&quot;&quot;&quot;+result[&#39;results&#39;][0][&#39;formatted_address&#39;]+&quot;&quot;&quot;cluster: &quot;&quot;&quot;+str(i)+&quot;&quot;&quot;&#39; });&quot;&quot;&quot; centro=&#39;{lat:&#39;+ str(np.mean(cosa[&#39;lat&#39;])) +&quot;&quot;&quot; , lng: &quot;&quot;&quot;+str(np.mean(cosa[&#39;lon&#39;]))+&#39;}&#39; hours=[&#39;00:00:00&#39;,&#39;00:15:00&#39;,&#39;00:30:00&#39;,&#39;00:45:00&#39;,&#39;01:00:00&#39;, &#39;01:15:00&#39;,&#39;01:30:00&#39;,&#39;01:45:00&#39;,&#39;02:00:00&#39;,&#39;02:15:00&#39;, &#39;02:30:00&#39;,&#39;02:45:00&#39;,&#39;03:00:00&#39;,&#39;03:15:00&#39;,&#39;03:30:00&#39;, &#39;03:45:00&#39;,&#39;04:00:00&#39;,&#39;04:15:00&#39;,&#39;04:30:00&#39;,&#39;04:45:00&#39;, &#39;05:00:00&#39;,&#39;05:15:00&#39;,&#39;05:30:00&#39;,&#39;05:45:00&#39;,&#39;06:00:00&#39;, &#39;06:15:00&#39;,&#39;06:30:00&#39;,&#39;06:45:00&#39;,&#39;07:00:00&#39;,&#39;07:15:00&#39;, &#39;07:30:00&#39;,&#39;07:45:00&#39;,&#39;08:00:00&#39;,&#39;08:15:00&#39;,&#39;08:30:00&#39;, &#39;08:45:00&#39;,&#39;09:00:00&#39;,&#39;09:15:00&#39;,&#39;09:30:00&#39;,&#39;09:45:00&#39;, &#39;10:00:00&#39;,&#39;10:15:00&#39;,&#39;10:30:00&#39;,&#39;10:45:00&#39;,&#39;11:00:00&#39;, &#39;11:15:00&#39;,&#39;11:30:00&#39;,&#39;11:45:00&#39;,&#39;12:00:00&#39;,&#39;12:15:00&#39;, &#39;12:30:00&#39;,&#39;12:45:00&#39;,&#39;13:00:00&#39;,&#39;13:15:00&#39;,&#39;13:30:00&#39;, &#39;13:45:00&#39;,&#39;14:00:00&#39;,&#39;14:15:00&#39;,&#39;14:30:00&#39;,&#39;14:45:00&#39;, &#39;15:00:00&#39;,&#39;15:15:00&#39;,&#39;15:30:00&#39;,&#39;15:45:00&#39;,&#39;16:00:00&#39;, &#39;16:15:00&#39;,&#39;16:30:00&#39;,&#39;16:45:00&#39;,&#39;17:00:00&#39;,&#39;17:15:00&#39;,&#39;17:30:00&#39;,&#39;17:45:00&#39;, &#39;18:00:00&#39;,&#39;18:15:00&#39;,&#39;18:30:00&#39;,&#39;18:45:00&#39;,&#39;19:00:00&#39;,&#39;19:15:00&#39;,&#39;19:30:00&#39;,&#39;19:45:00&#39;, &#39;20:00:00&#39;,&#39;20:15:00&#39;,&#39;20:30:00&#39;,&#39;20:45:00&#39;,&#39;21:00:00&#39;,&#39;21:15:00&#39;,&#39;21:30:00&#39;,&#39;21:45:00&#39;, &#39;22:00:00&#39;,&#39;22:15:00&#39;,&#39;22:30:00&#39;,&#39;22:45:00&#39;,&#39;23:00:00&#39;,&#39;23:15:00&#39;,&#39;23:30:00&#39;,&#39;23:45:00&#39;] result = pd.merge(coords2, intento,how=&#39;inner&#39;, on=[&#39;lat&#39;, &#39;lon&#39;]) join=result.drop_duplicates() bajo=&#39;00:00:00&#39; alto=&#39;00:00:00&#39; transporte=0 cluster=-20 ultima=np.min(join[&#39;hora&#39;]) for j in range(1,len(hours)): alto=hours[j] chin=join[(join[&#39;hora&#39;]&gt;bajo)&amp;(join[&#39;hora&#39;]&lt;alto)] if len(chin[&#39;cluster&#39;].unique())&gt;1: #print (&#39;Cambio de cluster!!&#39;) conflicto={} for i in chin[&#39;cluster&#39;].unique(): conflicto[i]=np.min(chin[chin[&#39;cluster&#39;]==i][&#39;hora&#39;]) sorted_x = sorted(conflicto.items(), key=operator.itemgetter(1)) print(sorted_x) for ii in sorted_x: i=ii[0] if cluster==i: ultima=np.max(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;]) else: print(&#39;te fuiste de &#39;,cluster,&#39; a las &#39;,ultima) auxT=ultima cluster=i print(&#39;llegaste a &#39;,cluster, &#39;a las &#39;,np.min(chin[chin[&#39;cluster&#39;]==i][&#39;hora&#39;])) ultima=np.max(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;]) print(&#39;--Tiempo de traslado: &#39;,(datetime.datetime.strptime(np.min(chin[chin[&#39;cluster&#39;]==i][&#39;hora&#39;]),&#39;%H:%M:%S&#39;)-datetime.datetime.strptime(auxT,&#39;%H:%M:%S&#39;)).seconds) transporte= transporte+(datetime.datetime.strptime(np.min(chin[chin[&#39;cluster&#39;]==i][&#39;hora&#39;]),&#39;%H:%M:%S&#39;)-datetime.datetime.strptime(auxT,&#39;%H:%M:%S&#39;)).seconds else: if len(chin[&#39;cluster&#39;].unique())==1: if cluster==chin[&#39;cluster&#39;].unique()[0]: ultima=np.max(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;]) else: if cluster==-20: print(&#39;amaneciste en&#39;, chin[&#39;cluster&#39;].unique()[0]) if len(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;])&gt;0: ultima=np.max(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;]) auxT=ultima print(ultima,auxT) else: #print(&#39;cambio de cluster &#39;,cluster,&#39; a &#39;,chin[&#39;cluster&#39;].unique()[0]) print(&#39;Te fuiste de &#39;,cluster,&#39; a las &#39;,ultima) auxT=ultima #print(&#39;Ultima ub. registrada: &#39;,np.max(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;])) # print(&#39;Ultima ub. registrada para &#39;,cluster,&#39;: &#39;,cluster,ultima) #print(chin[chin[&#39;cluster&#39;]==cluster]) cluster=chin[&#39;cluster&#39;].unique()[0] auxT=ultima ultima=np.max(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;]) print(&#39;Llegaste a &#39;,cluster,&#39; a las: &#39;,np.min(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;])) print(&#39;--Tiempo de traslado: &#39;,str(datetime.timedelta(seconds=(datetime.datetime.strptime(np.min(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;]),&#39;%H:%M:%S&#39;)-datetime.datetime.strptime(auxT,&#39;%H:%M:%S&#39;)).seconds))) transporte= transporte+(datetime.datetime.strptime(np.min(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;]),&#39;%H:%M:%S&#39;)-datetime.datetime.strptime(auxT,&#39;%H:%M:%S&#39;)).seconds #else: # print(&#39;no hay ubicaciones registradas entre &#39;,bajo,&#39; y &#39;,alto) bajo=alto print (&#39;En el día usaste&#39;,str(datetime.timedelta(seconds=transporte)),&#39; para desplazarte&#39;) Output: 0 1900-01-01 22:52:04.600000 1900-01-01 20:23:29.400000 Paseo de la Reforma 50, Miguel Hidalgo, 11550 Ciudad de México, CDMX, Mexico 1 1900-01-01 22:54:49.300000 1900-01-01 18:35:55.700000 Felipe Villanueva 19, Guadalupe Inn, 01020 Ciudad de México, CDMX, Mexico 2 1900-01-01 16:45:28.700000 1900-01-01 10:27:10.300000 Torre C, Av Sta Fe 505, Santa Fe, Contadero, 01219 Ciudad de México, CDMX, Mexico 3 1900-01-01 07:25:18.700000 1900-01-01 00:50:28.300000 Cerro San Francisco 309, Campestre Churubusco, 04200 Ciudad de México, CDMX, Mexico amaneciste en 3 Llegaste a 3 a las: 00:01:07 --Tiempo de traslado: 0:00:00 Te fuiste de 3 a las 08:14:40 Llegaste a 2 a las: 09:39:53 --Tiempo de traslado: 1:25:13 Te fuiste de 2 a las 17:32:46 Llegaste a 1 a las: 18:03:34 --Tiempo de traslado: 0:30:48 Te fuiste de 1 a las 19:48:47 Llegaste a 0 a las: 20:04:55 --Tiempo de traslado: 0:16:08 Te fuiste de 0 a las 23:10:39 Llegaste a 1 a las: 23:21:48 --Tiempo de traslado: 0:11:09 En el día usaste 2:23:18 para desplazarte clusters 4.4 Searches It is very important to get insights of the searches data because is going to help to find highlights keywords whose will deprecate the ites of interest the application is going to recommend. When we analyzed the user searches, we considered two approaches that allowed us to detect relevant characteristics and activities of the user, and to use this information obtained in the recommendation. First, to detect topics in a general way is to build a full corpus, where we can detect general characteristics of the user using automatic clustering algorithms, then compare the different algorithms outputs and determine the number of topics. Secondly, we seek to analyze the user search periods, once the most important issues are detected in the first step, to detect the periodicity in terms of frequency and search topics. In this way, we will be able to detect if the identified issues correspond to activities that the user still performs, that already has time without realizing them or that has been doing them for a long time, later we use this information in the algorithm of recommendation. In both cases an automatic detection is sought. To achieve these goals we consider the following characteristics of Google searches: They do not require grammatical rules to be able to show a result It is advisable to perform searches without punctuation marks to achieve better results Google is able to interpret languages without specifying the language in which you are writing Considering these characteristics, we observe that the steps for preprocessing the text decrease, and it is also necessary to perform preprocessing in different languages, in our case it will be done in English and Spanish, which are the most frequently used languages among our users. Although later research may include automatic language detection tools to perform automatic cleaning of the text, according to the corpus being processed. #!/usr/local/Cellar/python3/3.5.1/bin/python3 import sys import matplotlib.pyplot as plt import numpy as np import pandas as pd if __name__ == &quot;__main__&quot;: todos=[[] for i in range(7)] dias=0 while True: linea = sys.stdin.readline() if not linea: break # print(linea) separado=linea.split(&#39;,&#39;) # 1 es numeroDia, 2 es nombreDia x = int(separado[1]) y = [int(i) for i in separado[3:27]] #print(separado) #print(y) todos[x].append(y) dias+=1 fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(9, 4)) axes[0][0].set_title(&#39;Lunes&#39;) axes[0][1].set_title(&#39;Martes&#39;) axes[0][2].set_title(&#39;Miercoles&#39;) axes[0][3].set_title(&#39;Jueves&#39;) axes[1][0].set_title(&#39;Viernes&#39;) axes[1][1].set_title(&#39;Sabado&#39;) axes[1][2].set_title(&#39;Domingo&#39;) numdia = 0 for j in range(2): for i in range(4): if not (j == 1 and i &gt; 2): dia = np.array(todos[numdia]) numdia+=1 bar_l = [i+1 for i in range(24)] performance=dia.mean(0) error=dia.std(0) # axes[j][i].barh(bar_l, performance, xerr=error, align=&#39;center&#39;,alpha = 0.5, color=&#39;green&#39;, ecolor=&#39;gray&#39;) axes[j][i].errorbar(bar_l, performance, yerr=error, fmt=&#39;o&#39;) # adding horizontal grid lines #for ax in axes: # ax.yaxis.grid(True) # ax.set_xticks([y+1 for y in range(len(all_data))]) # ax.set_xlabel(&#39;xlabel&#39;) # ax.set_ylabel(&#39;ylabel&#39;) print(&quot;dias: {}&quot;.format(dias)) # add x-tick labels #plt.setp(axes, xticks=[y+1 for y in range(len(all_data))], # xticklabels=[&#39;x1&#39;, &#39;x2&#39;, &#39;x3&#39;, &#39;x4&#39;]) plt.show() search-frequency from datetime import timedelta, datetime import json import sys import operator def daterange(start_date, end_date): for n in range(int ((end_date - start_date).days)): yield start_date + timedelta(n) if __name__ == &quot;__main__&quot;: while True: x = sys.stdin.readline() x = x.replace(&#39;\\n&#39;, &#39;&#39;) if not x: break # print(x) # mostrar nombre del archivo datemin=datetime.now() datemax=datetime.fromtimestamp(0/1e6) with open(x) as data_file: data = json.load(data_file) dias = {} i=0 for query in data[&#39;event&#39;]: query_text = query[&#39;query&#39;][&#39;query_text&#39;] timestamp = int(query[&#39;query&#39;][&#39;id&#39;][0][&#39;timestamp_usec&#39;]) date = datetime.fromtimestamp(timestamp/1e6) nombredia = date.strftime(&quot;%A&quot;) diasemana = date.weekday() if date &gt; datemax: datemax=date if date &lt; datemin: datemin=date hash = date.year * 10000 + date.month * 100 + date.day if hash in dias.keys(): dias[hash][date.hour+2]+=1 else: dias[hash]=[0 for i in range(24)] dias[hash].insert(0,nombredia) dias[hash].insert(0,diasemana) dias[hash][date.hour+2]+=1 # print(&quot;num dias con consultas: {}&quot;.format(len(dias))) for date in daterange(datemin, datemax): hash = date.year * 10000 + date.month * 100 + date.day if not hash in dias.keys(): nombredia = date.strftime(&quot;%A&quot;) diasemana = date.weekday() dias[hash]=[0 for i in range(24)] dias[hash].insert(0,nombredia) dias[hash].insert(0,diasemana) #print(&quot;faltaba: {}&quot;.format(hash)) #print single_date.strftime(&quot;%Y-%m-%d&quot;) sorted_x = sorted(dias.items(), key=operator.itemgetter(0)) for k, v in enumerate(sorted_x): width = len(v[1]) for j in range(width): if j == 0: print(&#39;{},&#39;.format(v[0]), end=&#39;&#39;) if j == width-1: print(&#39;{}&#39;.format(v[1][j])) else: print(&#39;{},&#39;.format(v[1][j]), end=&#39;&#39;) "]
=======
["index.html", "Just a Recommender System Based on Google Applications Data Chapter 1 Abstract", " Just a Recommender System Based on Google Applications Data Alfredo, Eduardo, Alaín, Pedro V - ITAM 2017-04-15 Chapter 1 Abstract This study examined the development of a simple recommender system based on Google Takeout data. With the help of data from volunteers, we download the historical records of Gmail, Location and Searching from Google It was driven an exploratory data analysis in order to understand the nature of features. Then we predicted the most frequent places the individual has been using non supervised machine learning algorithms based on density. We found the network that the individual can be influenced by, and the important keywords used, to filter topics the person is likely to find out. The way to solve this problem was building an aplication on a beta version that the reader can easily download and try. After the statistical steps one can find out that sites of interest around the most visited places for a person, can be highly accuracy recommended based on searching histpry, and networking. # install.packages(&quot;devtool&quot;) # devtools::install_github(&quot;rstudio/bookdown&quot;) "],
["introduction.html", "Chapter 2 Introduction", " Chapter 2 Introduction As the number of items in markets increases, the quantity of available choices also increases. Consequently, users might face loads of irrelevant data when looking for an item. This problem could be felt when users face a large number of goods which can confuse them to choose what they are looking for, that is why recommender system have achive inmerse in consumer experience. One of the main motivatons of the study is reaching the different ways a person might have inssights based on the data we permited to Google to use. First of all we have to understant the data we are using, Google Takeout is the backup service provided to you through your Google Apps Account. You have to opt-in to Public Services in order to have access to this feature. The reeader have to make sure you are logged into your Google Apps account and then navigate to your Google Takeout interface. Google Takeout was created by the Google Data Liberation Front on June 28, 20111 to allow users to export their data from most of Google’s services. Since its creation, Google has added several more services to Takeout due to popular demand from user. The user can elect to export all of the available services or choose services from the above list. Takeout (Takeaway in other languages) will then process the request and put all the files into a zip file. Takeout then optionally sends an email notification that the export is completed, at which point the user can download the archive from the downloads section of the website. The zip file contains a separate folder for each service that was selected for export. We might say, one can acces our data, but we have to be careful because of the personal data rigts, in general whether information relates to an identified or identifiable individual 2, this should be always clear to understand we we use onter’s data. Because privacy may be an archaic term when used in reference to depositing information online. Unlike writing a note of secrecy and keeping it safely guarded inside a vault, keeping information hidden and secure online is radically different. We live in an age where we all feel like rulers to our information, kings and queens of bank accounts, yet we are not. It is clear to scope that data is a currency, and Recommender Systems are tools capable of predicting the preferences of users over sets of items (given the historical user-preference data). Recomender systems can be found almost everywhere in the digital space (e.g. Amazon, Google, Netflix), shaping the choices we make, the products we buy, the books we read, or movies we watch. Al those are typically produce a list of recommendations in one of two ways: Through collaborative and content-based filtering or the personality-based approach.3 Collaborative filtering approaches building a model from a user’s past behavior (items previously purchased or selected and/or numerical ratings given to those items) as well as similar decisions made by other users. This model is then used to predict items (or ratings for items) that the user may have an interest in. Moreover, this always provide the users with the best available choice. In this case a recommendation model can be applied in order to help users to find what they are looking for faster and easier. In general, the model analyzes the existing data to generate the recommendation list. For this study we are going to merge the data from people on the most common Google applications. As a result, users could find what they are looking for, on their favorites places around in a shorter time and desirable a higher precision. But well we have to work on that 1 2 3 "],
["google-takeout-data.html", "Chapter 3 Google Takeout Data 3.1 Search 3.2 Locations 3.3 E-mails", " Chapter 3 Google Takeout Data First let’s take a look on the data and its nature we will use, after downloading Google Takeout Data we have to be able to handle the data format. The format used for Search and Location is JSON there some approaches to use this kind of data, on [jq Manual] https://stedolan.github.io/jq/manual/ is very clear to see. Transformation of data from .mbox to format is quite more difficult because does not have a regular format, we defined where each mail is identified and separated for each one part of the same body, the we created a database in Neo4j to represent the network of the mails and identify the people with whom you have more interaction, finally we extracted keywords of the mails and to detect subjects in them. There’s some general observations used in order to explore the datasets. - The amount of data varies for each account. - For the cases explored, data are available since 2011. - The common field is the timestamp. Furthermore we have to settle some questions about how can we get of the data. 3.1 Search What insights can we get? How are searches by hour, day, month? Are there long search times? Productive searches? Do the most wanted words say anything? How the data looks like? {&quot;query&quot;: {&quot;id&quot;: [{&quot;timestamp_usec&quot;:&quot;1407774749032392&quot;}], &quot;query_text&quot;:&quot;banco mundial&quot;}} {&quot;query&quot;: {&quot;id&quot;: [{&quot;timestamp_usec&quot;:&quot;1407774749075527&quot;}], “query_text&quot;:&quot;data lake&quot;}} {&quot;query&quot;: {&quot;id&quot;: [{&quot;timestamp_usec&quot;:&quot;1407774749095273&quot;}], “query_text”:&quot;shiba dog&quot;}} 3.2 Locations What insights can we get? What is the frequency of movements? Can work and home be identified? When is a move identified? What are the average transfers in time and distance? How the data looks like? {&quot;timestampMs&quot;: &quot;1414819151315&quot;, &quot;latitudeE7&quot; : 204435729, &quot;longitudeE7&quot; : -872882348, &quot;accuracy&quot; : 49, &quot;activitys&quot; : [ { &quot;timestampMs&quot;: &quot;1414819136573&quot;, &quot;activities&quot; : [ { &quot;type&quot; : &quot;inVehicle&quot;, &quot;confidence&quot; : 62 }, { &quot;type&quot; : &quot;still&quot;, &quot;confidence&quot; : 29 }, { &quot;type&quot; : &quot;onBicycle&quot;, &quot;confidence&quot; : 5 }, { &quot;type&quot; : &quot;unknown&quot;, &quot;confidence&quot; : 5 } ] } ] } 3.3 E-mails What insights can we get? How is the traffic over time? Is it possible to make a network of people? What is the relationship of sent with received? Does the subject matter mean anything? How the data looks like? X-GM-THRID: 1545043292255087830 X-Gmail-Labels: Importante,Destacados,Recibidos From: &lt;ventasweb@interjet.com.mx&gt; To: &lt;xxx@gmail.com&gt; Reply-To: &lt;ventasweb@interjet.com.mx&gt; Date: Fri, 9 Sep 2016 19:41:43 -0500 Subject: Interjet Itinerario Content-Type: multipart/alternative; Message-ID: &lt;e34b7917-c506-4a84-ac90-626bf8fafb7a Content-Transfer-Encoding: quoted-printable —CONTENT— 3.3.1 Pipeline In order to conduct the study and the application development we have to try to answer those questions and have a scope to get information with data merged to make the recommender system accurate. We also might make a list of reacheable tasks: + Estimate a level of area as neighborhood or city where the user has lived or worked, let’s call the, Favorite Places. + Identify, through locations combined with searches, tastes or additional activities of the user. + Find a network related with searches + Estimate what the user is engaged in, by correlating emails and searches. + Make clusters in order to find places of interest + Generate recommendations based on the profile of what you consume daily + Frequencies for each location level for time window + Extract “zone” more frequently and define it as “residence city”. A high leve pipeline could be pipeline JSON "],
["exploratory-data-analysis.html", "Chapter 4 Exploratory Data Analysis 4.1 Network Analysis 4.2 E-mails Analysis 4.3 Locations 4.4 Searches", " Chapter 4 Exploratory Data Analysis Once the often laborious task of data manipulation is complete, the next step in the process is to become intimately familiar with the data set by performing Exploratory Data Analysis (EDA). The way to gain this level of understanding is to utilize the features of the statistical environment one is using that support this effort. It is always a good idea to explore a data set with multiple exploratory techniques, especially when they can be done together for comparison, we did not have a large amount of feaures but and important amount of effort to merge the data. Then, it is quite possible that one may need to revisit one or more data manipulation tasks in order to refine or transform the data even further. The goal of exploratory data analysis is to obtain confidence in the data to a point where one is ready to engage a machine learning algorithms. see EDA 4.1 Network Analysis What if becasuse of my networking one is tended to make different choices?, Is it the same relevance that one send ten e-mails to ten persons, than send ten e-mails to two persons? Well it is quite difficult to see at the first gaze, and maybe we have to make a deeper in-degrees and out-degrees analysis on e-mail networking. graph Let’s find the biggest email threads, once we got it, we visualize the longest path, using cypher query language Biggest email threads MATCH p=(e:Email)&lt;-[:REPLY*]-(r:Email)&lt;-[]-(sender:Account) WHERE NOT (e)-[:REPLY]-&gt;() RETURN sender.name, e.subject, Id(e), length(p) - 1 AS depth ORDER BY depth DESC LIMIT 100 // Email thread by email id MATCH p=(n:Email)&lt;-[:REPLY*]-(:Email) WHERE id(n)=123 RETURN p longest To achive a database based on graph we used a parser builded on Go programing language, helped on a .mbox reader builded in Perl A little summary of email2neo4jpackage: mbox2neo4j is go command line tools that allow you to import your emails from an mbox file into a neo4j graph database. It used a model from the email example project that is explained in Chapter 3 of the book Graph Databases by Ian Robinson, Jim Webber and Emil Eifrem (which is a truly great introduction to the graph btw) First we have to take the attachments of the e-mails On terminal: perl strip-attachments.pl /path/to/mbox/file.mbox Then mbox2neo4j /path/to/mbox/file localhost:7474 4.2 E-mails Analysis It is important to know the distribution of the emailing traffic, then is suitable to know when recommend, as we see before the more we know the user data the accurate the recommendation. The e-mail traffic analysis through the time is partially based on Geoff Boeing work, and it is been developed on python import mailbox, pandas as pd, numpy as np import matplotlib.pyplot as plt, matplotlib.font_manager as fm from dateutil.parser import parse as parse_datetime %matplotlib inline # define the fonts to use for plots #family = &#39;Myriad Pro&#39; family = &#39;serif&#39; title_font = fm.FontProperties(family=family, style=&#39;normal&#39;, size=20, weight=&#39;normal&#39;, stretch=&#39;normal&#39;) label_font = fm.FontProperties(family=family, style=&#39;normal&#39;, size=16, weight=&#39;normal&#39;, stretch=&#39;normal&#39;) ticks_font = fm.FontProperties(family=family, style=&#39;normal&#39;, size=12, weight=&#39;normal&#39;, stretch=&#39;normal&#39;) Load the Gmail archive and parse dates/times from messages # load the mbox file #path = &#39;Destacados.mbox&#39; path = &#39;/Users/pedrohserrano/google-takeout/Mail/Enviados.mbox&#39; mbox = mailbox.mbox(path) print(&#39;There are {:,} messages in the archive.&#39;.format(len(mbox))) There are 1,699 messages in the archive. The Gmail mbox file includes emails and hangouts chats among its “messages”. Hangouts messages don’t have date/time, so we’ll only parse dates and times from the actual emails, and just ignore the hangouts chats. Also, some chats do have a date. To filter them out, verify that if the message has a label that the label does not include “Chat”. # get a list of the dates/times of all the messages in the mbox all_dates = [] all_times = [] for message in mbox: # it&#39;s an email and not a chat if there&#39;s no label, or if there&#39;s a label but it&#39;s not &#39;chat&#39; if not &#39;X-Gmail-Labels&#39; in message or (&#39;X-Gmail-Labels&#39; in message and not &#39;Chat&#39; in message[&#39;X-Gmail-Labels&#39;]): if &#39;Date&#39; in message and message[&#39;Date&#39;] is not None: try: date, time = str(parse_datetime(message[&#39;Date&#39;])).split(&#39; &#39;) except Exception as e: print(e, message[&#39;Date&#39;]) all_dates.append(date) all_times.append(time) else: # hangouts messages have no Date key, so skip them pass print(&#39;There are {:,} messages with dates.&#39;.format(len(all_dates))) There are 1,699 messages with dates. Plot the mail traffic by date¶ # get the count per date date_counts = pd.Series(all_dates).value_counts().sort_index() print(&#39;There are {:,} dates with messages.&#39;.format(len(date_counts))) date_counts.head() # not every date necessarily has a message, so fill in missing dates in the range with zeros date_range = pd.date_range(start=min(all_dates), end=max(all_dates), freq=&#39;D&#39;) index = date_range.map(lambda x: str(x.date())) date_counts = date_counts.reindex(index, fill_value=0) print(&#39;There are {:,} dates total in the range, with or without messages.&#39;.format(len(date_counts))) date_counts.head() # create a series of labels for the plot: each new year&#39;s day xlabels = pd.Series([label if &#39;01-01&#39; in label else None for label in date_counts.index]) xlabels = xlabels[pd.notnull(xlabels)] xlabels.head() # plot the counts per day fig = plt.figure(figsize=[15, 5]) ax = date_counts.plot(kind=&#39;line&#39;, linewidth=0.5, alpha=0.5, color=&#39;g&#39;) ax.grid(True) ax.set_xticks(xlabels.index) ax.set_xticklabels(xlabels, rotation=35, rotation_mode=&#39;anchor&#39;, ha=&#39;right&#39;, fontproperties=ticks_font) ax.set_ylabel(&#39;Number of emails&#39;, fontproperties=label_font) ax.set_title(&#39;Sent mails traffic per day&#39;, fontproperties=title_font) fig.tight_layout() fig.savefig(&#39;images/gmail-traffic-day-destacados.png&#39;, dpi=96) plt.show() gmail-traffic-day-destacados Plot the traffic month by month # get the count per month all_months = [x[:-3] for x in all_dates] month_counts = pd.Series(all_months).value_counts().sort_index() # not every month necessarily has a message, so fill in missing months in the range with zeros date_range = pd.date_range(start=min(all_dates), end=max(all_dates), freq=&#39;D&#39;) months_range = date_range.map(lambda x: str(x.date())[:-3]) index = np.unique(months_range) month_counts = month_counts.reindex(index, fill_value=0) # create a series of labels for the plot: each january xlabels = pd.Series([label if &#39;-01&#39; in label else None for label in month_counts.index]) xlabels = xlabels[pd.notnull(xlabels)] xlabels.head() # plot the counts per month fig = plt.figure(figsize=[15, 5]) ax = month_counts.plot(kind=&#39;line&#39;, linewidth=2.5, alpha=0.6, color=&#39;g&#39;, marker=&#39;+&#39;, markeredgecolor=&#39;g&#39;) ax.grid(True) ax.set_xticks(xlabels.index) ax.set_xticklabels(xlabels, rotation=35, rotation_mode=&#39;anchor&#39;, ha=&#39;right&#39;, fontproperties=ticks_font) ax.set_ylabel(&#39;Number of emails&#39;, fontproperties=label_font) ax.set_title(&#39;Sent mail traffic per month&#39;, fontproperties=title_font) fig.tight_layout() fig.savefig(&#39;images/gmail-traffic-month.png&#39;, dpi=96) plt.show() gmail-traffic-month 4.2.1 Plot the mail traffic by the day of the week # get the count per day of the week day_counts = pd.DataFrame() day_counts[&#39;count&#39;] = date_counts day_counts[&#39;day_of_week&#39;] = date_counts.index.map(lambda x: parse_datetime(x).weekday()) mean_day_counts = day_counts.groupby(&#39;day_of_week&#39;)[&#39;count&#39;].mean() xlabels = [&#39;Monday&#39;, &#39;Tuesday&#39;, &#39;Wednesday&#39;, &#39;Thursday&#39;, &#39;Friday&#39;, &#39;Saturday&#39;, &#39;Sunday&#39;] fig = plt.figure(figsize=[15, 5]) ax = mean_day_counts.plot(kind=&#39;bar&#39;, width=0.6, alpha=0.5, color=&#39;g&#39;, edgecolor=&#39;#333333&#39;) ax.yaxis.grid(True) ax.set_xticklabels(xlabels, rotation=35, rotation_mode=&#39;anchor&#39;, ha=&#39;right&#39;, fontproperties=ticks_font) for label in ax.get_yticklabels(): label.set_fontproperties(ticks_font) ax.set_title(&#39;Sent mails traffic by day of the week&#39;, fontproperties=title_font) ax.set_xlabel(&#39;&#39;) ax.set_ylabel(&#39;Mean number of emails&#39;, fontproperties=label_font) fig.tight_layout() fig.savefig(&#39;images/gmail-traffic-day-week.png&#39;, dpi=96) plt.show() gmail-traffic-day-week 4.2.2 Plot the mail traffic by the hour of the day # get the count per hour of the day times = pd.Series(all_times).map(lambda x: &#39;{:02}:00&#39;.format(parse_datetime(x).hour)) time_counts = times.value_counts().sort_index() time_counts.head() fig = plt.figure(figsize=[15, 5]) ax = time_counts.plot(kind=&#39;bar&#39;, width=0.8, alpha=0.5, color=&#39;g&#39;, edgecolor=&#39;#333333&#39;) ax.yaxis.grid(True) ax.set_xticklabels(time_counts.index, rotation=45, rotation_mode=&#39;anchor&#39;, ha=&#39;right&#39;, fontproperties=ticks_font) for label in ax.get_yticklabels(): label.set_fontproperties(ticks_font) ax.set_title(&#39;Sent mails traffic by hour of the day&#39;, fontproperties=title_font) ax.set_ylabel(&#39;Number of emails&#39;, fontproperties=label_font) fig.tight_layout() fig.savefig(&#39;images/gmail-traffic-hour.png&#39;, dpi=96) plt.show() gmail-traffic-hour 4.2.3 Plot the mail traffic by the minute of the day # get the count per minute of the day, as hh:mm minutes = pd.Series(all_times).map(lambda x: &#39;{:02}:{:02}&#39;.format(parse_datetime(x).hour, parse_datetime(x).minute)) minute_counts = minutes.value_counts().sort_index() # not every minute necessarily has a message, so fill in missing times with zeros time_range = pd.date_range(start=&#39;0:00&#39;, end=&#39;23:59&#39;, freq=&#39;1min&#39;) index = time_range.map(lambda x: &#39;{:02}:{:02}&#39;.format(x.hour, x.minute)) minute_counts = minute_counts.reindex(index, fill_value=0) # create a series of labels for the plot: each new hour xlabels = pd.Series([label if &#39;:00&#39; in label else None for label in minute_counts.index]) xlabels = xlabels[pd.notnull(xlabels)] # plot the counts per minute fig = plt.figure(figsize=[15, 5]) ax = minute_counts.plot(kind=&#39;line&#39;, linewidth=0.7, alpha=0.7, color=&#39;g&#39;) ax.grid(True) ax.set_xticks(xlabels.index) ax.set_xticklabels(xlabels, rotation=45, rotation_mode=&#39;anchor&#39;, ha=&#39;right&#39;, fontproperties=ticks_font) ax.set_ylabel(&#39;Number of emails&#39;, fontproperties=label_font) ax.set_title(&#39;Sent mails traffic by minute of the day&#39;, fontproperties=title_font) fig.tight_layout() fig.savefig(&#39;images/gmail-traffic-minute.png&#39;, dpi=96) plt.show() gmail-traffic-hour As we can see here if we want to develop a recomender system around this user better we send ads based on how likely the user is online, we can now that when he/she is writing emails down For this user the most likely time is around 10 to eleven and 16 to 17, Tuesdays and Thursdays, and also is important to say that there’s such an active account between November and February. gmail-traffic-hour 4.3 Locations The main input we used is the location history file, that is because the the application is going to recomend the sites of interest around the most visited places. In other words we seek to describe the daily activity of the person and define their most frequent places (home, office or school). During the day, a person performs two types of activities: staying in one place or moving to another. When a person visits a place, they tend to stay at that location for at least 10 minutes (can be extended to hours in home, office or school cases) which generates at least 17 location records. When the person moves from one place to another, a series of location records are generated along the transited space. These records may or may not be close (geographically) depending on the transfer speed. If you are in a congestion, it is possible, although unlikely, to confuse the place as a visit (depending on the tolerance we take for each visit). As a result of the day, location records generate a cloud of points with areas of high and low density. In order to define the places visited during the day, it is proposed to use the DBSCAN Model which is an unsupervised clusterization algorithm based on density. When we used, the DBSCAN, unclassified points and points clusters are obtained. It will be assumed that points within a cluster are variations due to GPS accuracy or internal movements within the building. Also, within the clusters are the last records obtained during the displacement towards said cluster and the first records of the transfer to the next place. In order to define the visited location (the main one), the average of the records of each cluster will be taken excluding the first and the last decile in order to reduce the error caused by clustering. The unlabeled points correspond to transfer location records. Also was quite hard to define the frequently places, so we used a window of 30 days in order to detect if the period of analysis is a common period or a change of address (work or school) or a holiday period. Using the data of the volunteers for this study, we found outthat there are approximately 816 registered locations per day, approximately 34 records per hour or one record every 34 seconds. import numpy as np from sklearn.cluster import DBSCAN from sklearn import metrics #from sklearn.datasets.samples_generator import make_blobs from sklearn.preprocessing import StandardScaler import pandas as pd import json import simplejson import datetime import calendar from urllib.request import urlopen,quote import os import webbrowser #import operator #with open(&#39;Historialdeubicaciones.json&#39;, &#39;r&#39;) as fh: with open(&#39;LocationHistory2.json&#39;, &#39;r&#39;) as fh: raw = json.loads(fh.read()) ld = pd.DataFrame(raw[&#39;locations&#39;]) file = open(&quot;dia.csv&quot;,&quot;w&quot;) for i in range(len(ld)): file.write(&quot;{0:.7f}&quot;.format(ld[&#39;latitudeE7&#39;][i]/10000000)+&quot;,&quot;+&quot;{0:.7f}&quot;.format(ld[&#39;longitudeE7&#39;][i]/10000000)+&#39;,&#39;+ld[&#39;timestampMs&#39;][i]+&#39;,&#39;+ datetime.datetime.fromtimestamp( int(ld[&#39;timestampMs&#39;][i])/ 1e3 ).strftime(&#39;%Y-%m-%d&#39;)+&#39;,&#39;+datetime.datetime.fromtimestamp( int(ld[&#39;timestampMs&#39;][i])/ 1e3 ).strftime(&#39;%H:%M:%S&#39;) +&#39;,&#39; +calendar.day_name[datetime.datetime.fromtimestamp(int(ld[&#39;timestampMs&#39;][i])/ 1e3 ).weekday()]+ &#39;\\n&#39;) file.close() coords=pd.read_csv(&#39;dia.csv&#39;, names = [&quot;lat&quot;, &quot;lon&quot;,&quot;timestamp&quot;,&quot;fecha&quot;,&quot;hora&quot;,&quot;dia&quot;]) Define frequent places: 30 day window coords3=coords[coords.fecha==coords[&#39;fecha&#39;].unique()[0]] hours=[&#39;00:00:00&#39;,&#39;01:00:00&#39;, &#39;02:00:00&#39;,&#39;03:00:00&#39;, &#39;04:00:00&#39;,&#39;05:00:00&#39;, &#39;06:00:00&#39;,&#39;07:00:00&#39;, &#39;08:00:00&#39;,&#39;09:00:00&#39;, &#39;10:00:00&#39;,&#39;11:00:00&#39;, &#39;12:00:00&#39;,&#39;13:00:00&#39;, &#39;14:00:00&#39;,&#39;15:00:00&#39;, &#39;16:00:00&#39;,&#39;17:00:00&#39;, &#39;18:00:00&#39;,&#39;19:00:00&#39;, &#39;20:00:00&#39;,&#39;21:00:00&#39;, &#39;22:00:00&#39;,&#39;23:00:00&#39;,&#39;23:59:59&#39;] inicio=0 final=10 for i in range(inicio,final): coords3=coords3.append(coords[coords.fecha==coords[&#39;fecha&#39;].unique()[1+i]]) print(coords[&#39;fecha&#39;].unique()[1+inicio],coords[&#39;fecha&#39;].unique()[1+final]) cosa=coords3[[&#39;lat&#39;,&#39;lon&#39;]] cosa = cosa.reset_index(drop=True) min_samples=np.max([20,len(cosa)*.07]) scaler = StandardScaler() scaler.fit(cosa) X=scaler.fit_transform(cosa) direcciones={} db = DBSCAN(eps=0.031, min_samples=min_samples).fit(X) core_samples_mask = np.zeros_like(db.labels_, dtype=bool) core_samples_mask[db.core_sample_indices_] = True labels = db.labels_ cosa=cosa.iloc[db.core_sample_indices_] cosa = cosa.reset_index(drop=True) recuento={} # Number of clusters in labels, ignoring noise if present. n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0) import matplotlib.pyplot as plt unique_labels = set(labels) colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels))) clusters = [X[labels == i] for i in range(n_clusters_)] markers=&quot;&quot;&quot; &quot;&quot;&quot; places=&quot;AIzaSyCsgMwi_tzAVkae-8Rq9v2A_kjeJF5L2kU&quot; c0=scaler.inverse_transform(clusters[0]) c0r=pd.DataFrame(data=c0[0:,0:]) c0r.columns = [&#39;lat&#39;, &#39;lon&#39;] c0r[&#39;cluster&#39;]=0 casa={} matutino={} for i in range(n_clusters_): c0=scaler.inverse_transform(clusters[i]) c0r=pd.DataFrame(data=c0[0:,0:]) c0r.columns = [&#39;lat&#39;, &#39;lon&#39;] c0r[&#39;cluster&#39;]=i aux=c0r.drop_duplicates() aux=aux.reset_index(drop=True) horas=coords3[(coords3[&#39;lat&#39;]==aux[&#39;lat&#39;].loc[0]) &amp;(coords3[&#39;lon&#39;]==aux[&#39;lon&#39;].loc[0])] casa[i]=0 matutino[i]=0 vespertino=0 diurno=0 for k in range(1,len(aux)): horas=horas.append(coords3[(coords3[&#39;lat&#39;]==aux[&#39;lat&#39;].loc[k]) &amp;(coords3[&#39;lon&#39;]==aux[&#39;lon&#39;].loc[k])]) cosita=datetime.datetime.strptime(np.max(horas[&#39;hora&#39;]),&#39;%H:%M:%S&#39;)-datetime.datetime.strptime(np.min(horas[&#39;hora&#39;]),&#39;%H:%M:%S&#39;) maximo=datetime.datetime.strptime(np.max(horas[&#39;hora&#39;]),&#39;%H:%M:%S&#39;)-cosita*0 minimo=datetime.datetime.strptime(np.min(horas[&#39;hora&#39;]),&#39;%H:%M:%S&#39;)+cosita*0 bajo=&#39;00:00:00&#39; alto=&#39;00:00:00&#39; for alto in hours: temp=horas[(horas[&#39;hora&#39;]&lt;alto)&amp;(horas[&#39;hora&#39;]&gt;bajo)] recuento[alto]=len(temp[&#39;hora&#39;]) if ((alto&lt;&#39;07:00:00&#39;)|(alto&gt;&#39;23:00:00&#39;)): casa[i]=casa[i]+len(temp[&#39;hora&#39;]) if ((alto&lt;&#39;17:00:00&#39;)|(alto&gt;&#39;11:00:00&#39;)): matutino[i]=matutino[i]+len(temp[&#39;hora&#39;]) bajo=alto util=horas[(horas[&#39;hora&#39;]&lt;maximo.strftime(&quot;%H:%M:%S&quot;))&amp;(horas[&#39;hora&#39;]&gt;minimo.strftime(&quot;%H:%M:%S&quot;))] if(len(util)&gt;0): lon= np.mean(util[&#39;lon&#39;]) lat= np.mean(util[&#39;lat&#39;]) url_maps=&quot;https://maps.googleapis.com/maps/api/geocode/json?latlng=&quot;+str(lat)+&quot;,&quot;+str(lon)+&quot;&amp;key=AIzaSyCb0Wakn29V87eBdMd_fAb3DGcxAKtqtxY&quot; with urlopen(url_maps) as response: result= simplejson.load(urlopen(url_maps)) direcciones[i]=result[&#39;results&#39;][0][&#39;formatted_address&#39;] url_places1=&quot;https://maps.googleapis.com/maps/api/place/nearbysearch/json?location=&quot;+str(lat)+&quot;,&quot;+str(lon)+&quot;&amp;rankby=distance&quot;+&quot;&amp;types=None&quot;+&quot;&amp;key=&quot;+places markers=markers+&quot;&quot;&quot;var marker = new google.maps.Marker({ map: map, draggable: true, icon: { path: google.maps.SymbolPath.CIRCLE, scale:5 }, position: {lat: &quot;&quot;&quot;+ str(lat) +&quot;&quot;&quot; , lng: &quot;&quot;&quot;+str(lon)+&quot;&quot;&quot;}, title: &#39;&quot;&quot;&quot;+result[&#39;results&#39;][0][&#39;formatted_address&#39;]+&quot;&quot;&quot;cluster: &quot;&quot;&quot;+str(i)+&quot;&quot;&quot;&#39; });&quot;&quot;&quot; centro=&#39;{lat:&#39;+ str(np.mean(cosa[&#39;lat&#39;])) +&quot;&quot;&quot; , lng: &quot;&quot;&quot;+str(np.mean(cosa[&#39;lon&#39;]))+&#39;}&#39; print(casa) print(matutino) print(direcciones) 2017-03-21 2017-03-11 {0: 0, 1: 914, 2: 268, 3: 0} {0: 1386, 1: 3333, 2: 830, 3: 1040} {0: &#39;Edificio 10, Altavista, Ciudad de México, CDMX, Mexico&#39;, 1: &#39;Cerro San Francisco 305, Campestre Churubusco, 04200 Ciudad de México, CDMX, Mexico&#39;, 2: &#39;Cto. Interior Maestro José Vasconcelos 208, Condesa, 06140 Ciudad de México, CDMX, Mexico&#39;, 3: &#39;Torre C, Av Sta Fe 505, Santa Fe, Contadero, 01219 Ciudad de México, CDMX, Mexico&#39;} #print(&#39;Trabajo: &#39;,direcciones[max(matutino, key=matutino.get)]) aux=[k for k, v in casa.items() if v &gt; 0.4*sum(casa.values())] for i in aux: print(&#39;Casa &#39;,i,&#39;: &#39;,direcciones[i]) aux=[k for k, v in matutino.items() if v &gt; sum(matutino.values())/(n_clusters_+1)] for i in aux: print(&#39;Trabajo/Escuela &#39;,i,&#39;: &#39;,direcciones[i]) Casa 1 : Cerro San Francisco 305, Campestre Churubusco, 04200 Ciudad de México, CDMX, Mexico Trabajo/Escuela 0 : Edificio 10, Altavista, Ciudad de México, CDMX, Mexico Trabajo/Escuela 1 : Cerro San Francisco 305, Campestre Churubusco, 04200 Ciudad de México, CDMX, Mexico direcciones Day #with open(&#39;Historialdeubicaciones.json&#39;, &#39;r&#39;) as fh: with open(&#39;LocationHistory2.json&#39;, &#39;r&#39;) as fh: raw = json.loads(fh.read()) ld = pd.DataFrame(raw[&#39;locations&#39;]) file = open(&quot;dia.csv&quot;,&quot;w&quot;) for i in range(len(ld)): file.write(&quot;{0:.7f}&quot;.format(ld[&#39;latitudeE7&#39;][i]/10000000)+&quot;,&quot;+&quot;{0:.7f}&quot;.format(ld[&#39;longitudeE7&#39;][i]/10000000)+&#39;,&#39;+ld[&#39;timestampMs&#39;][i]+&#39;,&#39;+ datetime.datetime.fromtimestamp( int(ld[&#39;timestampMs&#39;][i])/ 1e3 ).strftime(&#39;%Y-%m-%d&#39;)+&#39;,&#39;+datetime.datetime.fromtimestamp( int(ld[&#39;timestampMs&#39;][i])/ 1e3 ).strftime(&#39;%H:%M:%S&#39;) +&#39;,&#39; +calendar.day_name[datetime.datetime.fromtimestamp(int(ld[&#39;timestampMs&#39;][i])/ 1e3 ).weekday()]+ &#39;\\n&#39;) file.close() coords=pd.read_csv(&#39;dia.csv&#39;, names = [&quot;lat&quot;, &quot;lon&quot;,&quot;timestamp&quot;,&quot;fecha&quot;,&quot;hora&quot;,&quot;dia&quot;]) coords2=coords[coords.fecha==&#39;2017-02-02&#39;] cosa=coords2[[&#39;lat&#39;,&#39;lon&#39;]] cosa = cosa.reset_index(drop=True) print(len(cosa)) min_samples=np.max([20,len(cosa)*.05]) print(&#39;min&#39;,min_samples) scaler = StandardScaler() scaler.fit(cosa) X=scaler.fit_transform(cosa) db = DBSCAN(eps=0.085, min_samples=min_samples).fit(X) core_samples_mask = np.zeros_like(db.labels_, dtype=bool) core_samples_mask[db.core_sample_indices_] = True print(len(db.core_sample_indices_)) labels = db.labels_ cosa=cosa.iloc[db.core_sample_indices_] cosa = cosa.reset_index(drop=True) # Number of clusters in labels, ignoring noise if present. n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0) print(&#39;Estimated number of clusters: %d&#39; % n_clusters_) import matplotlib.pyplot as plt unique_labels = set(labels) colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels))) for k, col in zip(unique_labels, colors): if k == -1: # Black used for noise. col = &#39;k&#39; class_member_mask = (labels == k) xy = X[class_member_mask &amp; core_samples_mask] plt.plot(xy[:, 0], xy[:, 1], &#39;o&#39;, markerfacecolor=col, markeredgecolor=&#39;k&#39;, markersize=7) xy = X[class_member_mask &amp; ~core_samples_mask] plt.plot(xy[:, 0], xy[:, 1], &#39;o&#39;, markerfacecolor=col, markeredgecolor=&#39;k&#39;, markersize=2) plt.title(&#39;Estimated number of clusters: %d&#39; % n_clusters_) plt.axis(&#39;off&#39;) plt.show() clusters = [X[labels == i] for i in range(n_clusters_)] markers=&quot;&quot;&quot; &quot;&quot;&quot; places=&quot;AIzaSyCsgMwi_tzAVkae-8Rq9v2A_kjeJF5L2kU&quot; c0=scaler.inverse_transform(clusters[0]) c0r=pd.DataFrame(data=c0[0:,0:]) c0r.columns = [&#39;lat&#39;, &#39;lon&#39;] c0r[&#39;cluster&#39;]=0 intento=c0r for i in range(n_clusters_): c0=scaler.inverse_transform(clusters[i]) c0r=pd.DataFrame(data=c0[0:,0:]) c0r.columns = [&#39;lat&#39;, &#39;lon&#39;] c0r[&#39;cluster&#39;]=i intento=intento.append(c0r) aux=c0r.drop_duplicates() aux=aux.reset_index(drop=True) horas=coords2[(coords2[&#39;lat&#39;]==aux[&#39;lat&#39;].loc[0]) &amp;(coords2[&#39;lon&#39;]==aux[&#39;lon&#39;].loc[0])] for k in range(1,len(aux)): horas=horas.append(coords2[(coords2[&#39;lat&#39;]==aux[&#39;lat&#39;].loc[k]) &amp;(coords2[&#39;lon&#39;]==aux[&#39;lon&#39;].loc[k])]) cosita=datetime.datetime.strptime(np.max(horas[&#39;hora&#39;]),&#39;%H:%M:%S&#39;)-datetime.datetime.strptime(np.min(horas[&#39;hora&#39;]),&#39;%H:%M:%S&#39;) maximo=datetime.datetime.strptime(np.max(horas[&#39;hora&#39;]),&#39;%H:%M:%S&#39;)-cosita*.1 minimo=datetime.datetime.strptime(np.min(horas[&#39;hora&#39;]),&#39;%H:%M:%S&#39;)+cosita*.1 print (i,maximo,minimo) util=horas[(horas[&#39;hora&#39;]&lt;maximo.strftime(&quot;%H:%M:%S&quot;))&amp;(horas[&#39;hora&#39;]&gt;minimo.strftime(&quot;%H:%M:%S&quot;))] if(len(util)&gt;0): lon= np.mean(util[&#39;lon&#39;]) lat= np.mean(util[&#39;lat&#39;]) url_maps=&quot;https://maps.googleapis.com/maps/api/geocode/json?latlng=&quot;+str(lat)+&quot;,&quot;+str(lon)+&quot;&amp;key=AIzaSyCb0Wakn29V87eBdMd_fAb3DGcxAKtqtxY&quot; with urlopen(url_maps) as response: result= simplejson.load(urlopen(url_maps)) print (result[&#39;results&#39;][0][&#39;formatted_address&#39;]) url_places1=&quot;https://maps.googleapis.com/maps/api/place/nearbysearch/json?location=&quot;+str(lat)+&quot;,&quot;+str(lon)+&quot;&amp;rankby=distance&quot;+&quot;&amp;types=None&quot;+&quot;&amp;key=&quot;+places #with urlopen(url_places1) as response: # result_p1= simplejson.load(urlopen(url_places1)) #print (&quot;están en:&quot;,result_p1[&#39;results&#39;][0][&#39;name&#39;],&#39;---&#39;,result_p1[&#39;results&#39;][0][&#39;types&#39;][0]) markers=markers+&quot;&quot;&quot;var marker = new google.maps.Marker({ map: map, draggable: true, icon: { path: google.maps.SymbolPath.CIRCLE, scale:5 }, position: {lat: &quot;&quot;&quot;+ str(lat) +&quot;&quot;&quot; , lng: &quot;&quot;&quot;+str(lon)+&quot;&quot;&quot;}, title: &#39;&quot;&quot;&quot;+result[&#39;results&#39;][0][&#39;formatted_address&#39;]+&quot;&quot;&quot;cluster: &quot;&quot;&quot;+str(i)+&quot;&quot;&quot;&#39; });&quot;&quot;&quot; centro=&#39;{lat:&#39;+ str(np.mean(cosa[&#39;lat&#39;])) +&quot;&quot;&quot; , lng: &quot;&quot;&quot;+str(np.mean(cosa[&#39;lon&#39;]))+&#39;}&#39; hours=[&#39;00:00:00&#39;,&#39;00:15:00&#39;,&#39;00:30:00&#39;,&#39;00:45:00&#39;,&#39;01:00:00&#39;, &#39;01:15:00&#39;,&#39;01:30:00&#39;,&#39;01:45:00&#39;,&#39;02:00:00&#39;,&#39;02:15:00&#39;, &#39;02:30:00&#39;,&#39;02:45:00&#39;,&#39;03:00:00&#39;,&#39;03:15:00&#39;,&#39;03:30:00&#39;, &#39;03:45:00&#39;,&#39;04:00:00&#39;,&#39;04:15:00&#39;,&#39;04:30:00&#39;,&#39;04:45:00&#39;, &#39;05:00:00&#39;,&#39;05:15:00&#39;,&#39;05:30:00&#39;,&#39;05:45:00&#39;,&#39;06:00:00&#39;, &#39;06:15:00&#39;,&#39;06:30:00&#39;,&#39;06:45:00&#39;,&#39;07:00:00&#39;,&#39;07:15:00&#39;, &#39;07:30:00&#39;,&#39;07:45:00&#39;,&#39;08:00:00&#39;,&#39;08:15:00&#39;,&#39;08:30:00&#39;, &#39;08:45:00&#39;,&#39;09:00:00&#39;,&#39;09:15:00&#39;,&#39;09:30:00&#39;,&#39;09:45:00&#39;, &#39;10:00:00&#39;,&#39;10:15:00&#39;,&#39;10:30:00&#39;,&#39;10:45:00&#39;,&#39;11:00:00&#39;, &#39;11:15:00&#39;,&#39;11:30:00&#39;,&#39;11:45:00&#39;,&#39;12:00:00&#39;,&#39;12:15:00&#39;, &#39;12:30:00&#39;,&#39;12:45:00&#39;,&#39;13:00:00&#39;,&#39;13:15:00&#39;,&#39;13:30:00&#39;, &#39;13:45:00&#39;,&#39;14:00:00&#39;,&#39;14:15:00&#39;,&#39;14:30:00&#39;,&#39;14:45:00&#39;, &#39;15:00:00&#39;,&#39;15:15:00&#39;,&#39;15:30:00&#39;,&#39;15:45:00&#39;,&#39;16:00:00&#39;, &#39;16:15:00&#39;,&#39;16:30:00&#39;,&#39;16:45:00&#39;,&#39;17:00:00&#39;,&#39;17:15:00&#39;,&#39;17:30:00&#39;,&#39;17:45:00&#39;, &#39;18:00:00&#39;,&#39;18:15:00&#39;,&#39;18:30:00&#39;,&#39;18:45:00&#39;,&#39;19:00:00&#39;,&#39;19:15:00&#39;,&#39;19:30:00&#39;,&#39;19:45:00&#39;, &#39;20:00:00&#39;,&#39;20:15:00&#39;,&#39;20:30:00&#39;,&#39;20:45:00&#39;,&#39;21:00:00&#39;,&#39;21:15:00&#39;,&#39;21:30:00&#39;,&#39;21:45:00&#39;, &#39;22:00:00&#39;,&#39;22:15:00&#39;,&#39;22:30:00&#39;,&#39;22:45:00&#39;,&#39;23:00:00&#39;,&#39;23:15:00&#39;,&#39;23:30:00&#39;,&#39;23:45:00&#39;] result = pd.merge(coords2, intento,how=&#39;inner&#39;, on=[&#39;lat&#39;, &#39;lon&#39;]) join=result.drop_duplicates() bajo=&#39;00:00:00&#39; alto=&#39;00:00:00&#39; transporte=0 cluster=-20 ultima=np.min(join[&#39;hora&#39;]) for j in range(1,len(hours)): alto=hours[j] chin=join[(join[&#39;hora&#39;]&gt;bajo)&amp;(join[&#39;hora&#39;]&lt;alto)] if len(chin[&#39;cluster&#39;].unique())&gt;1: #print (&#39;Cambio de cluster!!&#39;) conflicto={} for i in chin[&#39;cluster&#39;].unique(): conflicto[i]=np.min(chin[chin[&#39;cluster&#39;]==i][&#39;hora&#39;]) sorted_x = sorted(conflicto.items(), key=operator.itemgetter(1)) print(sorted_x) for ii in sorted_x: i=ii[0] if cluster==i: ultima=np.max(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;]) else: print(&#39;te fuiste de &#39;,cluster,&#39; a las &#39;,ultima) auxT=ultima cluster=i print(&#39;llegaste a &#39;,cluster, &#39;a las &#39;,np.min(chin[chin[&#39;cluster&#39;]==i][&#39;hora&#39;])) ultima=np.max(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;]) print(&#39;--Tiempo de traslado: &#39;,(datetime.datetime.strptime(np.min(chin[chin[&#39;cluster&#39;]==i][&#39;hora&#39;]),&#39;%H:%M:%S&#39;)-datetime.datetime.strptime(auxT,&#39;%H:%M:%S&#39;)).seconds) transporte= transporte+(datetime.datetime.strptime(np.min(chin[chin[&#39;cluster&#39;]==i][&#39;hora&#39;]),&#39;%H:%M:%S&#39;)-datetime.datetime.strptime(auxT,&#39;%H:%M:%S&#39;)).seconds else: if len(chin[&#39;cluster&#39;].unique())==1: if cluster==chin[&#39;cluster&#39;].unique()[0]: ultima=np.max(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;]) else: if cluster==-20: print(&#39;amaneciste en&#39;, chin[&#39;cluster&#39;].unique()[0]) if len(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;])&gt;0: ultima=np.max(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;]) auxT=ultima print(ultima,auxT) else: #print(&#39;cambio de cluster &#39;,cluster,&#39; a &#39;,chin[&#39;cluster&#39;].unique()[0]) print(&#39;Te fuiste de &#39;,cluster,&#39; a las &#39;,ultima) auxT=ultima #print(&#39;Ultima ub. registrada: &#39;,np.max(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;])) # print(&#39;Ultima ub. registrada para &#39;,cluster,&#39;: &#39;,cluster,ultima) #print(chin[chin[&#39;cluster&#39;]==cluster]) cluster=chin[&#39;cluster&#39;].unique()[0] auxT=ultima ultima=np.max(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;]) print(&#39;Llegaste a &#39;,cluster,&#39; a las: &#39;,np.min(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;])) print(&#39;--Tiempo de traslado: &#39;,str(datetime.timedelta(seconds=(datetime.datetime.strptime(np.min(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;]),&#39;%H:%M:%S&#39;)-datetime.datetime.strptime(auxT,&#39;%H:%M:%S&#39;)).seconds))) transporte= transporte+(datetime.datetime.strptime(np.min(chin[chin[&#39;cluster&#39;]==cluster][&#39;hora&#39;]),&#39;%H:%M:%S&#39;)-datetime.datetime.strptime(auxT,&#39;%H:%M:%S&#39;)).seconds #else: # print(&#39;no hay ubicaciones registradas entre &#39;,bajo,&#39; y &#39;,alto) bajo=alto print (&#39;En el día usaste&#39;,str(datetime.timedelta(seconds=transporte)),&#39; para desplazarte&#39;) Output: 0 1900-01-01 22:52:04.600000 1900-01-01 20:23:29.400000 Paseo de la Reforma 50, Miguel Hidalgo, 11550 Ciudad de México, CDMX, Mexico 1 1900-01-01 22:54:49.300000 1900-01-01 18:35:55.700000 Felipe Villanueva 19, Guadalupe Inn, 01020 Ciudad de México, CDMX, Mexico 2 1900-01-01 16:45:28.700000 1900-01-01 10:27:10.300000 Torre C, Av Sta Fe 505, Santa Fe, Contadero, 01219 Ciudad de México, CDMX, Mexico 3 1900-01-01 07:25:18.700000 1900-01-01 00:50:28.300000 Cerro San Francisco 309, Campestre Churubusco, 04200 Ciudad de México, CDMX, Mexico amaneciste en 3 Llegaste a 3 a las: 00:01:07 --Tiempo de traslado: 0:00:00 Te fuiste de 3 a las 08:14:40 Llegaste a 2 a las: 09:39:53 --Tiempo de traslado: 1:25:13 Te fuiste de 2 a las 17:32:46 Llegaste a 1 a las: 18:03:34 --Tiempo de traslado: 0:30:48 Te fuiste de 1 a las 19:48:47 Llegaste a 0 a las: 20:04:55 --Tiempo de traslado: 0:16:08 Te fuiste de 0 a las 23:10:39 Llegaste a 1 a las: 23:21:48 --Tiempo de traslado: 0:11:09 En el día usaste 2:23:18 para desplazarte clusters 4.4 Searches It is very important to get insights of the searches data because is going to help to find highlights keywords whose will deprecate the ites of interest the application is going to recommend. When we analyzed the user searches, we considered two approaches that allowed us to detect relevant characteristics and activities of the user, and to use this information obtained in the recommendation. First, to detect topics in a general way is to build a full corpus, where we can detect general characteristics of the user using automatic clustering algorithms, then compare the different algorithms outputs and determine the number of topics. Secondly, we seek to analyze the user search periods, once the most important issues are detected in the first step, to detect the periodicity in terms of frequency and search topics. In this way, we will be able to detect if the identified issues correspond to activities that the user still performs, that already has time without realizing them or that has been doing them for a long time, later we use this information in the algorithm of recommendation. In both cases an automatic detection is sought. To achieve these goals we consider the following characteristics of Google searches: They do not require grammatical rules to be able to show a result It is advisable to perform searches without punctuation marks to achieve better results Google is able to interpret languages without specifying the language in which you are writing Considering these characteristics, we observe that the steps for preprocessing the text decrease, and it is also necessary to perform preprocessing in different languages, in our case it will be done in English and Spanish, which are the most frequently used languages among our users. Although later research may include automatic language detection tools to perform automatic cleaning of the text, according to the corpus being processed. #!/usr/local/Cellar/python3/3.5.1/bin/python3 import sys import matplotlib.pyplot as plt import numpy as np import pandas as pd if __name__ == &quot;__main__&quot;: todos=[[] for i in range(7)] dias=0 while True: linea = sys.stdin.readline() if not linea: break # print(linea) separado=linea.split(&#39;,&#39;) # 1 es numeroDia, 2 es nombreDia x = int(separado[1]) y = [int(i) for i in separado[3:27]] #print(separado) #print(y) todos[x].append(y) dias+=1 fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(9, 4)) axes[0][0].set_title(&#39;Lunes&#39;) axes[0][1].set_title(&#39;Martes&#39;) axes[0][2].set_title(&#39;Miercoles&#39;) axes[0][3].set_title(&#39;Jueves&#39;) axes[1][0].set_title(&#39;Viernes&#39;) axes[1][1].set_title(&#39;Sabado&#39;) axes[1][2].set_title(&#39;Domingo&#39;) numdia = 0 for j in range(2): for i in range(4): if not (j == 1 and i &gt; 2): dia = np.array(todos[numdia]) numdia+=1 bar_l = [i+1 for i in range(24)] performance=dia.mean(0) error=dia.std(0) # axes[j][i].barh(bar_l, performance, xerr=error, align=&#39;center&#39;,alpha = 0.5, color=&#39;green&#39;, ecolor=&#39;gray&#39;) axes[j][i].errorbar(bar_l, performance, yerr=error, fmt=&#39;o&#39;) # adding horizontal grid lines #for ax in axes: # ax.yaxis.grid(True) # ax.set_xticks([y+1 for y in range(len(all_data))]) # ax.set_xlabel(&#39;xlabel&#39;) # ax.set_ylabel(&#39;ylabel&#39;) print(&quot;dias: {}&quot;.format(dias)) # add x-tick labels #plt.setp(axes, xticks=[y+1 for y in range(len(all_data))], # xticklabels=[&#39;x1&#39;, &#39;x2&#39;, &#39;x3&#39;, &#39;x4&#39;]) plt.show() search-frequency from datetime import timedelta, datetime import json import sys import operator def daterange(start_date, end_date): for n in range(int ((end_date - start_date).days)): yield start_date + timedelta(n) if __name__ == &quot;__main__&quot;: while True: x = sys.stdin.readline() x = x.replace(&#39;\\n&#39;, &#39;&#39;) if not x: break # print(x) # mostrar nombre del archivo datemin=datetime.now() datemax=datetime.fromtimestamp(0/1e6) with open(x) as data_file: data = json.load(data_file) dias = {} i=0 for query in data[&#39;event&#39;]: query_text = query[&#39;query&#39;][&#39;query_text&#39;] timestamp = int(query[&#39;query&#39;][&#39;id&#39;][0][&#39;timestamp_usec&#39;]) date = datetime.fromtimestamp(timestamp/1e6) nombredia = date.strftime(&quot;%A&quot;) diasemana = date.weekday() if date &gt; datemax: datemax=date if date &lt; datemin: datemin=date hash = date.year * 10000 + date.month * 100 + date.day if hash in dias.keys(): dias[hash][date.hour+2]+=1 else: dias[hash]=[0 for i in range(24)] dias[hash].insert(0,nombredia) dias[hash].insert(0,diasemana) dias[hash][date.hour+2]+=1 # print(&quot;num dias con consultas: {}&quot;.format(len(dias))) for date in daterange(datemin, datemax): hash = date.year * 10000 + date.month * 100 + date.day if not hash in dias.keys(): nombredia = date.strftime(&quot;%A&quot;) diasemana = date.weekday() dias[hash]=[0 for i in range(24)] dias[hash].insert(0,nombredia) dias[hash].insert(0,diasemana) #print(&quot;faltaba: {}&quot;.format(hash)) #print single_date.strftime(&quot;%Y-%m-%d&quot;) sorted_x = sorted(dias.items(), key=operator.itemgetter(0)) for k, v in enumerate(sorted_x): width = len(v[1]) for j in range(width): if j == 0: print(&#39;{},&#39;.format(v[0]), end=&#39;&#39;) if j == width-1: print(&#39;{}&#39;.format(v[1][j])) else: print(&#39;{},&#39;.format(v[1][j]), end=&#39;&#39;) "],
["building-a-recommender-system.html", "Chapter 5 Building a Recommender System 5.1 Modules", " Chapter 5 Building a Recommender System How is it? This is an application whose user interface is a shiny and backend mounted in python, everything is packaged in a docker compose instructions How does it work? It is based on 3 sources of information: google mails, locations and searches An unsupervised density-based machine learning algorithm is used to find clusters on a map, these clusters will be the most crowded places of the person, presumably: home, school, work, etc. Handing these important locations is the basis for showing (recommending) in the application the places of interest to, around those points with the help of the Google Maps API. But NOT all places of interest will be shown around these points, but they will be recommended only those sites that relate to keywords in their searches, and connections related to post The application has the option of being able to decide the time window in which it is desired to visualize What do the user expect? A Functional application with installation instructions: The user should be able to clone a repository (or pull the Dockerhub image) and run a docker container You will need to download your own Google Takeout files The user interface will give instructions for uploading the files The user visualizes which are their more crowded places through the time and its recommendations What does the application do? “in theory” Load the files Read the files Create location clusters Locate the most important ones as “My Favorites” (2 for example) 5. Find the keywords (10 for example) Filter sites of interest with keywords Shows the places of interest to recommend 5.1 Modules Luigi Pipeline Orchestration The pipeline consists of: A module for loading files (load.py) A module for reading files (read.py) A module for running the model (run.py) Docker Put everything into a docker compose image to be reproducible The docker is for the application, not for analysis, it should contain: python with packages used in locations and searches AWS "]
>>>>>>> a0743c88db250738f9b2bbea5243160e077dac12
]
